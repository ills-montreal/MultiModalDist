# ğŸŒŸ Vision Distillation

ğŸ“¦ This repository contains the implementation of our distillation method for vision modalities. The objective is to distill knowledge from multiple teacher embedders into a single student model.

## ğŸš€ Getting Started

### âœ… Prerequisites
- Python 3.10.14 ğŸ
- PyTorch ğŸ”¥
- torchvision ğŸ–¼ï¸

### ğŸ“‚ Dataset Preparation
Before training, you need to extract the embeddings of the dataset using the desired teacher models. The extracted embeddings must be saved in a specified directory.

**Available Datasets:**
- ğŸ“¸ CIFAR10
- ğŸŒ DTD
- ğŸ›©ï¸ STL10
- ğŸš¦ SVHN
- âœˆï¸ FGVCAircraft
- ğŸ¦ CUB

**Available Teacher Models:**
- ğŸŒ€ Swin
- ğŸ” ViT
- ğŸ“– BEiT
- ğŸ¦¾ DINOv2
- ğŸš€ PVTv2

### ğŸ‹ï¸â€â™‚ï¸ Training
After extracting embeddings, run the training script:

```bash
python multiDist/train_gm.py --modalities-to-simulate vision \
                             --embeddings-dir [EMBEDDINGS_PATH] \
                             --vision-student [STUDENT_MODEL] \
                             --vision-embedders-to-simulate [MODEL_NAME]
```

Example:

```bash
python multiDist/train_gm.py --modalities-to-simulate vision \
                             --embeddings-dir ./embeddings/ \
                             --vision-student Swin \
                             --vision-embedders-to-simulate ViT DINOv2
```

### ğŸ› ï¸ Model Options
- **Student Models:** ğŸŒ€ Swin, ğŸ” ViT, ğŸ“– BEiT, ğŸ¦¾ DINOv2, ğŸš€ PVTv2
- **Teacher Models:** ğŸŒ€ Swin, ğŸ” ViT, ğŸ“– BEiT, ğŸ¦¾ DINOv2, ğŸš€ PVTv2
